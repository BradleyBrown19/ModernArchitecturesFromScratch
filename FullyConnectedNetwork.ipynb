{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp fully_connected_network_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromScratch.basic_operations_01 import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near zero: 0.00012300178059376776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "xt,yt,xv,yv = get_mnist()\n",
    "\n",
    "n_out = (yt.max()+1).item()\n",
    "n_in = (xt[0].shape)[0]\n",
    "test_near_zero(xt.mean())\n",
    "show_im(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Kaiming initialization with fan_in\n",
    "def get_weight(in_d, out_d, relu_after):\n",
    "    \"Returns weight matrix of size `in_d` x `out_d` initialized using Kaiming initialization\"\n",
    "    if relu_after: return torch.randn(in_d, out_d) * math.sqrt(2. / in_d)\n",
    "    else: return torch.randn(in_d, out_d) / math.sqrt(in_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For full information of Kaiming initialization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2841, -0.9401, -0.4223, -0.7115,  0.5532],\n",
       "        [-0.1580,  0.7566, -0.7379,  0.7424,  0.1979],\n",
       "        [ 0.0763,  0.7963,  0.1251, -0.9009, -0.6070],\n",
       "        [-0.2342, -0.1588,  1.0073, -0.2527, -0.7623]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weight(4,5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "#hide\n",
    "def linear(x, w, b): \n",
    "    \"Basic linear layer\"\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def relu(x): \n",
    "    \"ReLU activation function\"\n",
    "    return x.clamp_min(0.) - 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def lin_rel(x, w, b): \n",
    "    \"Linear layer followed by ReLU activation on `x` with weight `w` and bias `b`\"\n",
    "    return relu(linear(x, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def softmax(x): \n",
    "    \"Softmax activation function\"\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "n_in = 28**2\n",
    "n_out = 10\n",
    "\n",
    "w1 = get_weight(n_in, 50, True)\n",
    "w2 = get_weight(50, n_out, False)\n",
    "b1 = torch.zeros(50)\n",
    "b2 = torch.zeros(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def forward_pass(xb):\n",
    "    xb = lin_rel(xb, w1, b1)\n",
    "    xb = linear(xb, w2, b2)\n",
    "    return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "result = forward_pass(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.013458955101668835\n",
      "Std: 0.8483681082725525\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "get_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def mse_loss(xb, yb): \n",
    "    \"Mean Square Error loss\"\n",
    "    return (xb.squeeze(-1) - yb).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "#hide\n",
    "def cross_entropy(xb, targ): \n",
    "    \"Cross Entropy Loss\"\n",
    "    return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backwards Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def mse_grad(inp, targ):\n",
    "    inp.g = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def rel_grad(inp, out):\n",
    "    inp.g = out.g * (inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def softmax_cross_grad(inp, targ):\n",
    "    targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)\n",
    "    inp_s = softmax(inp)\n",
    "    inp.g = ( inp_s - targ ) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def full_pass(xb, targ):\n",
    "    l1 = linear(xb, w1, b1)\n",
    "    l1_r = relu(l1)\n",
    "    l2 = linear(l1_r, w2, b2)\n",
    "    \n",
    "    soft = softmax(l2)\n",
    "    \n",
    "    loss = cross_entropy(soft, targ)\n",
    "    \n",
    "    softmax_cross_grad(l2, targ)\n",
    "    lin_grad(l1_r, l2, w2, b2)\n",
    "    rel_grad(l1, l1_r)\n",
    "    lin_grad(xb, l1, w1, b1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6825)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "full_pass(xt, yt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing vs Pytorch Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig  = xt.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "xt2 = xt.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def auto_full_pass(xb, targ):\n",
    "    l1 = linear(xb, w12, b12)\n",
    "    l1_r = relu(l1)\n",
    "    l2 = linear(l1_r, w22, b22)\n",
    "    soft = softmax(l2)\n",
    "    \n",
    "    loss = cross_entropy(soft, targ)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "loss = auto_full_pass(xt2, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, ig )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module():\n",
    "    \"Base class for every layer operation in a sequential network\"\n",
    "    def __call__(self, *args):\n",
    "        \"Executes forward pass of module and stores result in `self.out` for backwards pass\"\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): \n",
    "        \"Executes desired operation of module\"\n",
    "        raise Exception(\"Not Implemented\")\n",
    "        \n",
    "    def backward(self): \n",
    "        \"Calls backwards method to find gradient with stored output of layer\"\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.__call__\" class=\"doc_header\"><code>Module.__call__</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.__call__</code>(**\\*`args`**)\n",
       "\n",
       "Executes forward pass of module and stores result in `self.out` for backwards pass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.forward\" class=\"doc_header\"><code>Module.forward</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.forward</code>()\n",
       "\n",
       "Executes desired operation of module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.backward\" class=\"doc_header\"><code>Module.backward</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.backward</code>()\n",
       "\n",
       "Calls backwards method to find gradient with stored output of layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Module.__call__)\n",
    "show_doc(Module.forward)\n",
    "show_doc(Module.backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_d, out_d, final): \n",
    "        \"Initialize weight using 'get_weight' and bias to 0 for linear operation\"\n",
    "        self.w, self.b = get_weight(in_d, out_d, final), torch.zeros(out_d)\n",
    "\n",
    "    def forward(self, xb): \n",
    "        \"Perform forward linear pass\"\n",
    "        return xb @ self.w + self.b\n",
    "\n",
    "    def bwd(self, out, inp):\n",
    "        \"Gradient with respect to the forward linear layer\"\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.__init__\" class=\"doc_header\"><code>Linear.__init__</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.__init__</code>(**`in_d`**, **`out_d`**, **`final`**)\n",
       "\n",
       "Initialize weight using 'get_weight' and bias to 0 for linear operation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.forward\" class=\"doc_header\"><code>Linear.forward</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.forward</code>(**`xb`**)\n",
       "\n",
       "Perform forward linear pass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.bwd\" class=\"doc_header\"><code>Linear.bwd</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.bwd</code>(**`out`**, **`inp`**)\n",
       "\n",
       "Gradient with respect to the forward linear layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Linear.__init__)\n",
    "show_doc(Linear.forward)\n",
    "show_doc(Linear.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReLU(Module):\n",
    "    def forward(self, x): \n",
    "        \"Set all activations to have a minimum of zero, subtract 0.5 to maintain mean of 0\"\n",
    "        return x.clamp_min_(0.)-0.5\n",
    "    \n",
    "    def bwd(self, out, inp): \n",
    "        \"Backward with respect to the ReLU layer\"\n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ReLU.forward\" class=\"doc_header\"><code>ReLU.forward</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ReLU.forward</code>(**`x`**)\n",
       "\n",
       "Set all activations to have a minimum of zero, subtract 0.5 to maintain mean of 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ReLU.bwd\" class=\"doc_header\"><code>ReLU.bwd</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ReLU.bwd</code>(**`out`**, **`inp`**)\n",
       "\n",
       "Backward with respect to the ReLU layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ReLU.forward)\n",
    "show_doc(ReLU.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "eps = 1e-9\n",
    "class CrossSoft(Module):\n",
    "    def forward(self, inp, targ):\n",
    "        \"Calls `soft_forward` and `cross_loss` on inp compared with `targ`\"\n",
    "        softed = self.soft_forward(inp)\n",
    "        return self.cross_loss(softed, targ)\n",
    "    \n",
    "    def soft_forward(self, x): \n",
    "        \"Implements softmax activation function on `x`\"\n",
    "        return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)\n",
    "        \n",
    "    def cross_loss(self, xb, targ): \n",
    "        \"Cross entropy loss of `xb` compared to `targ`\"\n",
    "        return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )\n",
    "    \n",
    "    def bwd(self, loss, inp, targ):\n",
    "        \"Gradient with respect to both softmax and cross entropy loss\"\n",
    "        targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)\n",
    "        inp_s = softmax(inp)\n",
    "        inp.g = ( inp_s - targ ) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.forward\" class=\"doc_header\"><code>CrossSoft.forward</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.forward</code>(**`inp`**, **`targ`**)\n",
       "\n",
       "Calls `soft_forward` and `cross_loss` on inp compared with `targ`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.soft_forward\" class=\"doc_header\"><code>CrossSoft.soft_forward</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.soft_forward</code>(**`x`**)\n",
       "\n",
       "Implements softmax activation function on `x`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.cross_loss\" class=\"doc_header\"><code>CrossSoft.cross_loss</code><a href=\"__main__.py#L13\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.cross_loss</code>(**`xb`**, **`targ`**)\n",
       "\n",
       "Cross entropy loss of `xb` compared to `targ`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.bwd\" class=\"doc_header\"><code>CrossSoft.bwd</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.bwd</code>(**`loss`**, **`inp`**, **`targ`**)\n",
       "\n",
       "Gradient with respect to both softmax and cross entropy loss"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CrossSoft.forward)\n",
    "show_doc(CrossSoft.soft_forward)\n",
    "show_doc(CrossSoft.cross_loss)\n",
    "show_doc(CrossSoft.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "class Model():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Output and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(784,50,True), ReLU(), Linear(50,10, False)]\n",
    "loss_func = CrossSoft()\n",
    "model = Model(layers)\n",
    "loss = loss_func(model(xt),yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func.backward()\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = model.layers[0].w.g.clone()\n",
    "w2g = model.layers[2].w.g.clone()\n",
    "b1g = model.layers[0].b.g.clone()\n",
    "b2g = model.layers[2].b.g.clone()\n",
    "ig  = xt.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = xt.clone().requires_grad_(True)\n",
    "model.layers[0].w = model.layers[0].w.clone().requires_grad_(True)\n",
    "model.layers[0].b = model.layers[0].b.clone().requires_grad_(True)\n",
    "model.layers[2].w = model.layers[2].w.clone().requires_grad_(True)\n",
    "model.layers[2].b = model.layers[2].b.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 201 ms, sys: 21.6 ms, total: 223 ms\n",
      "Wall time: 55.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = loss_func(model(xt), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 827 ms, sys: 50.6 ms, total: 878 ms\n",
      "Wall time: 129 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "test_near(w2g, model.layers[2].w.grad)\n",
    "test_near(b2g, model.layers[2].b.grad)\n",
    "test_near(w1g, model.layers[0].w.grad)\n",
    "test_near(b1g, model.layers[0].b.grad)\n",
    "test_near(ig, xt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
