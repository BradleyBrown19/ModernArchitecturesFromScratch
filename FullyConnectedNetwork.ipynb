{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp fully_connected_network_02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Network\n",
    "> Implementing backward and forward passes to train a simple fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromScratch.basic_operations_01 import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near zero: 0.00012300178059376776\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "xt,yt,xv,yv = get_mnist()\n",
    "\n",
    "n_out = (yt.max()+1).item()\n",
    "n_in = (xt[0].shape)[0]\n",
    "test_near_zero(xt.mean())\n",
    "show_im(xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "#Kaiming initialization with fan_in\n",
    "def get_weight(in_d, out_d, relu_after):\n",
    "    \"Returns weight matrix of size `in_d` x `out_d` initialized using Kaiming initialization\"\n",
    "    if relu_after: return torch.randn(in_d, out_d) * math.sqrt(2. / in_d)\n",
    "    else: return torch.randn(in_d, out_d) / math.sqrt(in_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please see: https://arxiv.org/abs/1502.01852 for more details and explanation on Kaiming initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3439, -0.8647, -0.3632, -0.4948,  0.1206],\n",
       "        [ 1.1526,  0.0320, -0.1779,  0.0031, -0.5851],\n",
       "        [-0.7234,  0.7781, -0.2957, -0.2883, -0.0954],\n",
       "        [ 0.8227, -0.4535, -0.2321,  0.4181,  0.5786]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_weight(4,5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export   \n",
    "#hide\n",
    "def linear(x, w, b): \n",
    "    \"Basic linear layer\"\n",
    "    return x @ w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def relu(x): \n",
    "    \"ReLU activation function\"\n",
    "    return x.clamp_min(0.) - 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def lin_rel(x, w, b): \n",
    "    \"Linear layer followed by ReLU activation on `x` with weight `w` and bias `b`\"\n",
    "    return relu(linear(x, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def softmax(x): \n",
    "    \"Softmax activation function\"\n",
    "    return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "n_in = 28**2\n",
    "n_out = 10\n",
    "\n",
    "w1 = get_weight(n_in, 50, True)\n",
    "w2 = get_weight(50, n_out, False)\n",
    "b1 = torch.zeros(50)\n",
    "b2 = torch.zeros(n_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def forward_pass(xb):\n",
    "    xb = lin_rel(xb, w1, b1)\n",
    "    xb = linear(xb, w2, b2)\n",
    "    return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "result = forward_pass(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: -0.12942148745059967\n",
      "Std: 1.041248083114624\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "get_stats(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "def mse_loss(xb, yb): \n",
    "    \"Mean Square Error loss\"\n",
    "    return (xb.squeeze(-1) - yb).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-9\n",
    "#hide\n",
    "def cross_entropy(xb, targ): \n",
    "    \"Cross Entropy Loss\"\n",
    "    return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backwards Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    \"Grad for mean squared error\"\n",
    "    inp.g = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_grad(inp, out):\n",
    "    \"Grad for ReLU layer\"\n",
    "    inp.g = out.g * (inp > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b):\n",
    "    \"Grad for linear layer\"\n",
    "    inp.g = out.g @ w.t()\n",
    "    w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)\n",
    "    b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_cross_grad(inp, targ):\n",
    "    \"Grad for softmax and cross entropy loss\"\n",
    "    targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)\n",
    "    inp_s = softmax(inp)\n",
    "    inp.g = ( inp_s - targ ) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pass(xb, targ):\n",
    "    l1 = linear(xb, w1, b1)\n",
    "    l1_r = relu(l1)\n",
    "    l2 = linear(l1_r, w2, b2)\n",
    "    \n",
    "    soft = softmax(l2)\n",
    "    \n",
    "    loss = cross_entropy(soft, targ)\n",
    "    \n",
    "    softmax_cross_grad(l2, targ)\n",
    "    lin_grad(l1_r, l2, w2, b2)\n",
    "    rel_grad(l1, l1_r)\n",
    "    lin_grad(xb, l1, w1, b1)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9962)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = full_pass(xt, yt)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig  = xt.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "xt2 = xt.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "def auto_full_pass(xb, targ):\n",
    "    l1 = linear(xb, w12, b12)\n",
    "    l1_r = relu(l1)\n",
    "    l2 = linear(l1_r, w22, b22)\n",
    "    soft = softmax(l2)\n",
    "    \n",
    "    loss = cross_entropy(soft, targ)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "loss = auto_full_pass(xt2, yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, ig )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refactoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module():\n",
    "    \"Base class for every layer operation in a sequential network\"\n",
    "    def __call__(self, *args):\n",
    "        \"Executes forward pass of module and stores result in `self.out` for backwards pass\"\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    \n",
    "    def forward(self): \n",
    "        \"Executes desired operation of module\"\n",
    "        raise Exception(\"Not Implemented\")\n",
    "        \n",
    "    def backward(self): \n",
    "        \"Calls backwards method to find gradient with stored output of layer\"\n",
    "        self.bwd(self.out, *self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.__call__\" class=\"doc_header\"><code>Module.__call__</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.__call__</code>(**\\*`args`**)\n",
       "\n",
       "Executes forward pass of module and stores result in `self.out` for backwards pass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.forward\" class=\"doc_header\"><code>Module.forward</code><a href=\"__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.forward</code>()\n",
       "\n",
       "Executes desired operation of module"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Module.backward\" class=\"doc_header\"><code>Module.backward</code><a href=\"__main__.py#L14\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Module.backward</code>()\n",
       "\n",
       "Calls backwards method to find gradient with stored output of layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Module.__call__)\n",
    "show_doc(Module.forward)\n",
    "show_doc(Module.backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_d, out_d, final): \n",
    "        \"Initialize weight using 'get_weight' and bias to 0 for linear operation\"\n",
    "        self.w, self.b = get_weight(in_d, out_d, final), torch.zeros(out_d)\n",
    "\n",
    "    def forward(self, xb): \n",
    "        \"Perform forward linear pass\"\n",
    "        return xb @ self.w + self.b\n",
    "\n",
    "    def bwd(self, out, inp):\n",
    "        \"Gradient with respect to the forward linear layer\"\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.__init__\" class=\"doc_header\"><code>Linear.__init__</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.__init__</code>(**`in_d`**, **`out_d`**, **`final`**)\n",
       "\n",
       "Initialize weight using 'get_weight' and bias to 0 for linear operation"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.forward\" class=\"doc_header\"><code>Linear.forward</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.forward</code>(**`xb`**)\n",
       "\n",
       "Perform forward linear pass"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Linear.bwd\" class=\"doc_header\"><code>Linear.bwd</code><a href=\"__main__.py#L11\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Linear.bwd</code>(**`out`**, **`inp`**)\n",
       "\n",
       "Gradient with respect to the forward linear layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Linear.__init__)\n",
    "show_doc(Linear.forward)\n",
    "show_doc(Linear.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ReLU(Module):\n",
    "    def forward(self, x): \n",
    "        \"Set all activations to have a minimum of zero, subtract 0.5 to maintain mean of 0\"\n",
    "        return x.clamp_min_(0.)-0.5\n",
    "    \n",
    "    def bwd(self, out, inp): \n",
    "        \"Backward with respect to the ReLU layer\"\n",
    "        inp.g = (inp>0).float() * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ReLU.forward\" class=\"doc_header\"><code>ReLU.forward</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ReLU.forward</code>(**`x`**)\n",
       "\n",
       "Set all activations to have a minimum of zero, subtract 0.5 to maintain mean of 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"ReLU.bwd\" class=\"doc_header\"><code>ReLU.bwd</code><a href=\"__main__.py#L7\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>ReLU.bwd</code>(**`out`**, **`inp`**)\n",
       "\n",
       "Backward with respect to the ReLU layer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(ReLU.forward)\n",
    "show_doc(ReLU.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "eps = 1e-9\n",
    "class CrossSoft(Module):\n",
    "    def forward(self, inp, targ):\n",
    "        \"Calls `soft_forward` and `cross_loss` on inp compared with `targ`\"\n",
    "        softed = self.soft_forward(inp)\n",
    "        return self.cross_loss(softed, targ)\n",
    "    \n",
    "    def soft_forward(self, x): \n",
    "        \"Implements softmax activation function on `x`\"\n",
    "        return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)\n",
    "        \n",
    "    def cross_loss(self, xb, targ): \n",
    "        \"Cross entropy loss of `xb` compared to `targ`\"\n",
    "        return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )\n",
    "    \n",
    "    def bwd(self, loss, inp, targ):\n",
    "        \"Gradient with respect to both softmax and cross entropy loss\"\n",
    "        targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)\n",
    "        inp_s = softmax(inp)\n",
    "        inp.g = ( inp_s - targ ) / targ.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.forward\" class=\"doc_header\"><code>CrossSoft.forward</code><a href=\"__main__.py#L4\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.forward</code>(**`inp`**, **`targ`**)\n",
       "\n",
       "Calls `soft_forward` and `cross_loss` on inp compared with `targ`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.soft_forward\" class=\"doc_header\"><code>CrossSoft.soft_forward</code><a href=\"__main__.py#L9\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.soft_forward</code>(**`x`**)\n",
       "\n",
       "Implements softmax activation function on `x`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.cross_loss\" class=\"doc_header\"><code>CrossSoft.cross_loss</code><a href=\"__main__.py#L13\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.cross_loss</code>(**`xb`**, **`targ`**)\n",
       "\n",
       "Cross entropy loss of `xb` compared to `targ`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"CrossSoft.bwd\" class=\"doc_header\"><code>CrossSoft.bwd</code><a href=\"__main__.py#L17\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>CrossSoft.bwd</code>(**`loss`**, **`inp`**, **`targ`**)\n",
       "\n",
       "Gradient with respect to both softmax and cross entropy loss"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(CrossSoft.forward)\n",
    "show_doc(CrossSoft.soft_forward)\n",
    "show_doc(CrossSoft.cross_loss)\n",
    "show_doc(CrossSoft.bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#hide\n",
    "class Model():\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "        for l in reversed(self.layers): l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Output and Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(784,50,True), ReLU(), Linear(50,10, False)]\n",
    "loss_func = CrossSoft()\n",
    "model = Model(layers)\n",
    "loss = loss_func(model(xt),yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func.backward()\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = model.layers[0].w.g.clone()\n",
    "w2g = model.layers[2].w.g.clone()\n",
    "b1g = model.layers[0].b.g.clone()\n",
    "b2g = model.layers[2].b.g.clone()\n",
    "ig  = xt.g.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = xt.clone().requires_grad_(True)\n",
    "model.layers[0].w = model.layers[0].w.clone().requires_grad_(True)\n",
    "model.layers[0].b = model.layers[0].b.clone().requires_grad_(True)\n",
    "model.layers[2].w = model.layers[2].w.clone().requires_grad_(True)\n",
    "model.layers[2].b = model.layers[2].b.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 227 ms, sys: 24.3 ms, total: 251 ms\n",
      "Wall time: 62.9 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss = loss_func(model(xt), yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 796 ms, sys: 48.3 ms, total: 845 ms\n",
      "Wall time: 124 ms\n"
     ]
    }
   ],
   "source": [
    "%time loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "test_near(w2g, model.layers[2].w.grad)\n",
    "test_near(b2g, model.layers[2].b.grad)\n",
    "test_near(w1g, model.layers[0].w.grad)\n",
    "test_near(b1g, model.layers[0].b.grad)\n",
    "test_near(ig, xt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
