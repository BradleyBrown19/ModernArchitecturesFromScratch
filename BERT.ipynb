{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromPyTorch.nb_ScratchToPytorch import *\n",
    "import math\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "> Implementing Google's famous BERT transformer architecture. The most popular NLP model over the past few years that uses transformers to train in a bidirectional manner by taking advantage of masking tokens at train time.\n",
    "> Paper: https://arxiv.org/abs/1810.04805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, num_heads, emb_size, key_size, Activation=nn.ReLU, drop_p=0.5):\n",
    "        \"Transformer Layer: (Multiheaded) attention followed my linear layers\"\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.key_size = key_size\n",
    "        self.out_size = num_heads * key_size\n",
    "\n",
    "        self.query = nn.Linear(emb_size, self.out_size)\n",
    "        self.key = nn.Linear(emb_size, self.out_size)\n",
    "        self.value = nn.Linear(emb_size, self.out_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.combine_heads = nn.Linear(self.out_size, emb_size)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_size)\n",
    "        self.layer_norm_final = nn.LayerNorm(emb_size)\n",
    "\n",
    "        self.fcn = nn.Sequential(nn.Linear(emb_size, emb_size), Activation(), nn.Linear(emb_size, emb_size))\n",
    "\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        query = self.query(xb)\n",
    "        key = self.key(xb)\n",
    "        value = self.value(xb)\n",
    "\n",
    "        attention = query @ key.transpose(-1, -2)\n",
    "        scaled_attention = attention / math.sqrt(self.key_size)\n",
    "        normalized_attention = self.softmax(scaled_attention)\n",
    "        values = normalized_attention @ value\n",
    "\n",
    "        final_attention = self.layer_norm(self.drop(self.combine_heads(values)))\n",
    "\n",
    "        fcn_output = self.fcn(final_attention)\n",
    "\n",
    "        skip = self.drop(fcn_output) + xb\n",
    "\n",
    "        return self.layer_norm_final(skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, num_heads, emb_size, key_size, Activation=nn.ReLU, drop_p=0.5):\n",
    "        \"Transformer Layer: (Multiheaded) attention followed my linear layers\"\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.key_size = key_size\n",
    "        self.out_size = num_heads * key_size\n",
    "\n",
    "        self.query = nn.Linear(emb_size, self.out_size)\n",
    "        self.key = nn.Linear(emb_size, self.out_size)\n",
    "        self.value = nn.Linear(emb_size, self.out_size)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.combine_heads = nn.Linear(self.out_size, emb_size)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(emb_size)\n",
    "        self.layer_norm_final = nn.LayerNorm(emb_size)\n",
    "\n",
    "        self.fcn = nn.Sequential(nn.Linear(emb_size, emb_size), Activation(), nn.Linear(emb_size, emb_size))\n",
    "\n",
    "        self.drop = nn.Dropout(drop_p)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        query = self.query(xb)\n",
    "        key = self.key(xb)\n",
    "        value = self.value(xb)\n",
    "\n",
    "        attention = query @ key.transpose(-1, -2)\n",
    "        scaled_attention = attention / math.sqrt(self.key_size)\n",
    "        normalized_attention = self.softmax(scaled_attention)\n",
    "        values = normalized_attention @ value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, depth, max_seq_size, vocab_size):\n",
    "        \"Custom BERT embeddings, combination of positional, normal token embeddings and segment embeddings for decoding tasks\"\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_seq_size, depth)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, depth)\n",
    "        self.segment_embedding = nn.Embedding(2, depth)\n",
    "\n",
    "    \n",
    "    def forward(self, xb, token_types=None):\n",
    "        token_types = torch.zeros(xb.shape).long() if token_types is None else token_types\n",
    "        return self.pos_embedding(token_types) + self.token_embedding(xb) + self.segment_embedding(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class BERTEmbeddings(nn.Module):\n",
    "    def __init__(self, depth, max_seq_size, vocab_size):\n",
    "        \"Custom BERT embeddings, combination of positional, normal token embeddings and segment embeddings for decoding tasks\"\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_seq_size, depth)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, depth)\n",
    "        self.segment_embedding = nn.Embedding(2, depth)\n",
    "\n",
    "    \n",
    "    def forward(self, xb, token_types=None):\n",
    "        token_types = torch.zeros(xb.shape).long() if token_types is None else token_types\n",
    "        return self.pos_embedding(token_types) + self.token_embedding(xb) + self.segment_embedding(xb)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, emb_size, key_size, max_seq_size, vocab_size, **kwargs):\n",
    "        \"BERT in all it's glory\"\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbeddings(emb_size, max_seq_size, vocab_size) \n",
    "        transformer_layers = [TransformerLayer(num_heads, emb_size, key_size, **kwargs) for _ in range(num_layers)]\n",
    "        self.encoder = nn.Sequential(*transformer_layers)\n",
    "    \n",
    "    def forward(self, xb, token_types=None):\n",
    "        embeddings = self.embedding(xb, token_types)\n",
    "        output = self.encoder(embeddings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, num_layers, num_heads, emb_size, key_size, max_seq_size, vocab_size, **kwargs):\n",
    "        \"BERT in all it's glory\"\n",
    "        super().__init__()\n",
    "        self.embedding = BERTEmbeddings(emb_size, max_seq_size, vocab_size) \n",
    "        transformer_layers = [TransformerLayer(num_heads, emb_size, key_size, **kwargs) for _ in range(num_layers)]\n",
    "        self.encoder = nn.Sequential(*transformer_layers)\n",
    "    \n",
    "    def forward(self, xb, token_types=None):\n",
    "        embeddings = self.embedding(xb, token_types)\n",
    "        output = self.encoder(embeddings)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BERT(3, 3, 10, 5, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 10, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert(torch.zeros(16, 10).long()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file 'notebook2script.py': [Errno 2] No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py BERT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
