{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp scratch_to_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import fastai\n",
    "from fastai import datasets\n",
    "from fastai.vision import *\n",
    "import pdb\n",
    "import time\n",
    "import fire\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "from nbdev.showdoc import show_doc\n",
    "from ModernArchitecturesFromScratch.callbacks_05 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScratchToPytorch\n",
    "> The goal for this project from this point on is to shift away from implementing everything from scratch and using things that we've already built from PyTorch to take advantage of the C speed that makes us able to perform our own experiments and iterate quickly\n",
    "\n",
    "*NOTE: We will only be using PyTorch features that we have already built from scratch ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_imaggenette():\n",
    "    path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)\n",
    "    tfms = get_transforms()\n",
    "    return (ImageList.from_folder(path).split_by_rand_pct(0.2).label_from_folder().transform(tfms, size=224).databunch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CustomLearner():\n",
    "    def __init__(self, model, loss_func, optimizer, db):\n",
    "        \"Wrapper for model, loss function, optimizer and databunch\"\n",
    "        self.model, self.loss_func, self.optimizer, self.db = model, loss_func, optimizer(model.parameters()), db\n",
    "\n",
    "def get_learner(model, data=get_imaggenette(), loss=nn.CrossEntropyLoss(), optimizer=optim.Adam):\n",
    "    return CustomLearner(model, loss, optimizer, data)\n",
    "\n",
    "def get_runner(learn, callbacks:List): return Runner(learn, callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ProgressCallback(Callback):\n",
    "    \"Callback to make a nice progress bar with metrics for training. Slightly modified version of: https://github.com/fastai/course-v3/blob/master/nbs/dl2/09c_add_progress_bar.ipynb\"\n",
    "    _order=-1\n",
    "    def before_fit(self):\n",
    "        self.mbar = master_bar(range(self.epochs))\n",
    "        self.mbar.on_iter_begin()\n",
    "        self.runner.logger = partial(self.mbar.write, table=True)\n",
    "\n",
    "    def after_fit(self): self.mbar.on_iter_end()\n",
    "    def after_batch(self): self.pb.update(self.iters_done)\n",
    "    def before_train(self): self.set_pb(self.runner.databunch.train_dl)\n",
    "    def before_valid(self): self.set_pb(self.runner.databunch.valid_dl)\n",
    "\n",
    "    def set_pb(self, dl):\n",
    "        self.pb = progress_bar(dl, parent=self.mbar)\n",
    "        self.mbar.update(self.epoch)\n",
    "        self.pb.update(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TrainEvalCallback(Callback):\n",
    "    \"Keeps track of training/eval mode of model and progress through training\"\n",
    "    _order = 10\n",
    "\n",
    "    def before_fit(self):\n",
    "        #self.runner.opt = self.learner.optimizer(self.learner.model.parameters(), self.lr)\n",
    "        self.runner.epochs_done = 0.\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.runner.iters_done += 1\n",
    "        self.runner.epochs_done += 1/self.iters\n",
    "\n",
    "    def before_valid(self):\n",
    "        self.model.training = False\n",
    "\n",
    "    def before_train(self):\n",
    "        self.model.training = True\n",
    "\n",
    "    def after_epoch(self):\n",
    "        self.runner.iters_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def accuracy(out, yb, *args): return (torch.argmax(out, dim=1)==yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    def before_fit(self): self.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Runner():\n",
    "    \"All encompossing class to train a model with specific callbacks\"\n",
    "    def __init__(self, learner, cbs=None):\n",
    "        cbs = [] if cbs is None else cbs\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "\n",
    "        for cb in self.cbs:\n",
    "            cb.runner = self\n",
    "\n",
    "        self.learner = learner\n",
    "\n",
    "    @property\n",
    "    def model(self): return self.learner.model\n",
    "    @property\n",
    "    def optimizer(self): return self.learner.optimizer\n",
    "    @property\n",
    "    def loss_func(self): return self.learner.loss_func\n",
    "    @property\n",
    "    def databunch(self): return self.learner.db\n",
    "    @property\n",
    "    def dl(self): return self.learner.db.train_dl if self.model.training else self.learner.db.valid_dl\n",
    "\n",
    "    def do_one_batch(self, xb, yb):\n",
    "        \"Applies forward and backward passes of model to one batch\"\n",
    "\n",
    "        self.pred = self.learner.model(xb)\n",
    "        \n",
    "        self.loss = self.learner.loss_func(self.pred, yb)\n",
    "        if self.check_callbacks('after_loss') or not self.learner.model.training: return\n",
    "\n",
    "        self.loss.backward()\n",
    "        if self.check_callbacks('after_loss_back'): return\n",
    "\n",
    "        self.optimizer.step()\n",
    "        if self.check_callbacks('after_opt'): return\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.check_callbacks('after_zero_grad'): return\n",
    "\n",
    "    def do_all_batches(self, dl):\n",
    "        \"Runs every batch of a dataloader through `do_one_batch`\"\n",
    "        self.iters, self.iters_done = len(dl), 0\n",
    "        for xb, yb in dl:\n",
    "            if self.stop: break\n",
    "            self.xb, self.yb = xb,yb\n",
    "            if self.check_callbacks('before_batch'): return\n",
    "            self.do_one_batch(self.xb,self.yb)\n",
    "            if self.check_callbacks('after_batch'): return\n",
    "        self.iters = 0\n",
    "\n",
    "        self.stop = False\n",
    "\n",
    "    def fit(self, epochs, lr=0.1):\n",
    "        \"Method to fit the model `epoch` times using learning rate `lr`\"\n",
    "        self.optimizer.lr, self.epochs = lr, epochs\n",
    "        if self.check_callbacks('before_fit'): return\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.epoch = epoch\n",
    "            if self.check_callbacks('before_epoch'): return\n",
    "            \n",
    "            if not self.check_callbacks('before_train'): self.do_all_batches(self.learner.db.train_dl)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if not self.check_callbacks('before_valid'): self.do_all_batches(self.learner.db.valid_dl)\n",
    "\n",
    "            if self.check_callbacks('after_epoch'): break\n",
    "\n",
    "        if self.check_callbacks('after_fit'): return\n",
    "\n",
    "    def check_callbacks(self, state):\n",
    "        \"Helper functions to run through each callback, calling it's state method if applicable\"\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, state, None)\n",
    "            if f and f(): return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted ScratchToPytorch.ipynb to ModernArchitecturesFromPyTorch/nb_ScratchToPytorch.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py ScratchToPytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
