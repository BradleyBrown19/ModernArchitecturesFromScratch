{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp batchnorm_06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromScratch.basic_operations_01 import *\n",
    "from ModernArchitecturesFromScratch.fully_connected_network_02 import *\n",
    "from ModernArchitecturesFromScratch.model_training_03 import *\n",
    "from ModernArchitecturesFromScratch.convolutions_pooling_04 import *\n",
    "from ModernArchitecturesFromScratch.callbacks_05 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm\n",
    "> Implementing batchnorm regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CheckGrad(Callback):\n",
    "    _order = 100\n",
    "    def require_grad(self):\n",
    "        #pdb.set_trace()\n",
    "        for p in self.runner.model.parameters():\n",
    "            p.d.requires_grad_(True)\n",
    "    \n",
    "    def after_loss(self): \n",
    "        #pdb.set_trace()\n",
    "        if self.iters_done < 2: \n",
    "            #run.check1 = [self.runner.xb, self.runner.yb, self.pred] + [m.d.clone() for m in self.model.parameters()]\n",
    "            return\n",
    "        else:\n",
    "            #run.check2 = [self.runner.xb, self.runner.yb, self.pred] + [m.d.clone() for m in self.model.parameters()]\n",
    "            self.loss.backward()\n",
    "            self.runner.pytorch_gradients = [m.d.grad for m in self.runner.model.parameters()]\n",
    "            return True\n",
    "            \n",
    "    def after_model_back(self): \n",
    "        #pdb.set_trace()\n",
    "        return True\n",
    "    \n",
    "    def before_batch(self):\n",
    "        #pdb.set_trace()\n",
    "        if self.iters_done < 2: \n",
    "            self.copy_batch_x = self.runner.xb.clone()\n",
    "            self.copy_batch_y = self.runner.yb.clone()\n",
    "        else: \n",
    "            self.runner.xb = self.copy_batch_x\n",
    "            self.runner.yb = self.copy_batch_y\n",
    "            self.runner.xb.requires_grad_(True)\n",
    "            return\n",
    "    \n",
    "    def after_batch(self): \n",
    "        #pdb.set_trace()\n",
    "        if self.iters_done < 2: \n",
    "            self.runner.custom_gradients = [m.grad.clone() for m in self.runner.model.parameters()]\n",
    "            self.require_grad()\n",
    "            return\n",
    "        else: \n",
    "            return True\n",
    "        \n",
    "    def before_valid(self): return True\n",
    "    def after_epoch(self): return True\n",
    "    def after_fit(self):\n",
    "        #for i in range(len(run.check1)):\n",
    "            #test_near(run.check1[i], run.check2[i])\n",
    "        for i in range(len(self.runner.custom_gradients)):\n",
    "            test_near(self.runner.pytorch_gradients[i], self.runner.custom_gradients[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing CheckGrad on linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Runner(get_learner(), [CheckGrad()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Layer1): Linear(784, 50)\n",
       "(Layer2): ReLU()\n",
       "(Layer3): Linear(50, 10)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Batchnorm(Module):\n",
    "    \"Module for applying batch normalization\"\n",
    "    def __init__(self, nf, mom=0.1, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mom, self.eps = mom, eps\n",
    "        self.multiplier = Parameter(torch.ones(1,nf, 1, 1))\n",
    "        self.adder = Parameter(torch.zeros(1,nf,1,1))\n",
    "        self.means = torch.zeros(1,nf,1,1)\n",
    "        self.vars = torch.ones(1,nf,1,1)\n",
    "    \n",
    "    def update(self, xb):\n",
    "        #Get the mean and standard deviation of the batch, update running average\n",
    "        mean = xb.mean(dim=(0,2,3), keepdim=True)\n",
    "        var = xb.std(dim=(0,2,3), keepdim=True)\n",
    "        self.mean = self.mom * self.means + (1-self.mom) * mean\n",
    "        self.vars = self.mom * self.vars + (1-self.mom) * var\n",
    "        return mean, var\n",
    "        \n",
    "    \n",
    "    def forward(self, xb): \n",
    "        mean, var = self.update(xb)\n",
    "        self.after_stats = (xb - mean) / (var + self.eps).sqrt()\n",
    "        self.after_scaling = self.after_stats * self.multiplier.d + self.adder.d\n",
    "        return self.after_scaling\n",
    "        \n",
    "    def bwd(self, out, inp):\n",
    "        bs = out.g.shape[0]\n",
    "        \n",
    "        self.multiplier.update((out.g * self.after_stats).sum(dim=(0,2,3), keepdim=True))\n",
    "        self.adder.update(out.g.sum(dim=(0,2,3), keepdim=True))\n",
    "        \n",
    "        var_factor = 1./(self.vars+self.eps).sqrt()\n",
    "        mean_factor = inp - self.means\n",
    "        \n",
    "        delta_norm = out.g * self.multiplier.d\n",
    "        \n",
    "        delta_var = delta_norm * mean_factor * -0.5 * (self.vars + self.eps)**(-3/2)\n",
    "        delta_mean = delta_norm * -var_factor + delta_var * 1 / bs * -2 * mean_factor\n",
    "         \n",
    "        inp.g = (delta_norm * var_factor) + (delta_mean / bs) + (delta_var * 2 / bs * mean_factor)\n",
    "    \n",
    "    def __repr__(self): return f'Batchnorm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_conv_model():\n",
    "    return SequentialModel(Reshape(1, 28, 28),\n",
    "            Conv(1, 8, stride=2),\n",
    "            Batchnorm(8),\n",
    "            Flatten(),\n",
    "            Linear(1352, 10, False)\n",
    "    )\n",
    "\n",
    "def get_conv_learner():\n",
    "    m = get_conv_model()\n",
    "    o = Optimizer\n",
    "    l = CrossEntropy()\n",
    "    db = Databunch(*get_small_datasets())\n",
    "    return Learner(m,l,o,db)\n",
    "\n",
    "def get_conv_runner(callbacks):\n",
    "    learn = get_conv_learner()\n",
    "    run = Runner(learn, callbacks)\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = get_conv_runner([ProgressCallback(), CheckGrad()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n",
      "good\n",
      "good\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "run.fit(1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
