# AUTOGENERATED! DO NOT EDIT! File to edit: TrainingLoop.ipynb (unless otherwise specified).

__all__ = ['log_softmax', 'logsumexp', 'log_softmax_improved', 'nll', 'cross_entropy', 'accuracy', 'SequentialModel',
           'Parameter', 'Module', 'CrossEntropy', 'Linear', 'ReLU', 'Optimizer', 'get_model', 'Dataset', 'Batcher',
           'collate', 'DataLoader', 'fit', 'get_datasets']

# Cell
from .basic_operations_01 import *
from .fully_connected_network_02 import *

# Cell
def log_softmax(inp): return (inp.exp() / inp.exp().sum(-1, keepdim=True)).log()

def logsumexp(inp):
    a = inp.max(dim=1).values
    return a + ((inp-a[:,None]).exp().sum(-1)).log()

def log_softmax_improved(inp):
    return inp - logsumexp(inp).unsqueeze(-1)

# Cell
def nll(inp, targ): return -(inp[range(targ.shape[0]), targ.long()].mean())

# Cell
def cross_entropy(inp, targ): return nll((log_softmax_improved(inp)), targ)

# Cell
def accuracy(preds, targ): return (torch.argmax(preds, dim=1)==targ).float().mean()

# Cell
class SequentialModel():
    def __init__(self, *args):
        self.layers = list(args)
        self.training = True

    def __repr__(self):
        res = ["(Layer" + str(i+1) + "): " + str(m) for i,m in enumerate(self.layers)]
        return "\n".join(res)

    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

    def backward(self):
        for l in reversed(self.layers): l.backward()

    def parameters(self):
        for l in self.layers:
            for p in l.parameters(): yield p

# Cell

class Parameter():
    def __init__(self, data=None, requires_grad=True):
        self.d = torch.Tensor() if data is None else data
        self.requires_grad = requires_grad
        self.grad = 0.

    def step(self, lr):
        self.d -= lr * self.grad

    def zero_grad(self): self.grad = 0.

    def update(self, amount): self.grad = amount

    def __get__(self, instance, owner): return self.d

    def __repr__(self):
        return f'Parameter: {self.d.shape}, Requires grad: {self.requires_grad}'

# Cell

class Module():
    def __init__(self):
        self._params = {}

    def __call__(self, *args):
        self.args = args
        self.out = self.forward(*args)
        return self.out

    def forward(self): raise Exception("Not Implemented")

    def backward(self): self.bwd(self.out, *self.args)

    def __setattr__(self,k,v):
        if isinstance(v, Parameter): self._params[k] = v
        super().__setattr__(k,v)

    def parameters(self):
        for p in self._params.values(): yield p

# Cell

class CrossEntropy(Module):
    def forward(self, inp, targ):
        return cross_entropy(inp, targ)

    def bwd(self, loss, inp, targ):
        inp_s = softmax(inp)
        inp_s[range(targ.shape[0]), targ.long()] -= 1
        inp.g = inp_s / targ.shape[0]

# Cell

class Linear(Module):
    def __init__(self, in_d, out_d, relu_after, req_grad=True):
        super().__init__()
        self.w = Parameter(get_weight(in_d, out_d, relu_after), req_grad)
        self.b = Parameter(torch.zeros(out_d), req_grad)

    def forward(self, xb): return xb @ self.w.d + self.b.d

    def bwd(self, out, inp):
        inp.g = out.g @ self.w.d.t()
        self.w.update(inp.t() @ out.g)
        self.b.update(out.g.sum(0))

    def __repr__(self): return f'Linear({self.w.d.shape[0]}, {self.w.d.shape[1]})'

# Cell

class ReLU(Module):
    def forward(self, x): return x.clamp_min_(0.)-0.5
    def bwd(self, out, inp):
        inp.g = (inp>0).float() * out.g
    def __repr__(self): return f'ReLU()'

# Cell
class Optimizer():
    def __init__(self, params, lr): self.params, self.lr = list(params), lr

    def step(self):
        for p in self.params: p.step(self.lr)

    def zero_grad(self):
        for p in self.params: p.zero_grad()

# Cell
def get_model(lr):
    model = SequentialModel(Linear(784, 50, True), ReLU(), Linear(50, 10, False))
    loss_func = CrossEntropy()
    optimizer = Optimizer(model.parameters(), lr)
    return model, optimizer, loss_func

# Cell
class Dataset():

    def __init__(self, x, y): self.x, self.y = x, y

    def __getitem__(self, i): return self.x[i], self.y[i]

    def __len__(self): return len(self.x)

# Cell
class Batcher():
    def __init__(self, ds, bs, random): self.n, self.bs, self.rand = len(ds), bs, random
    def __iter__(self):
        self.idxs = torch.randperm(self.n) if self.rand else torch.arange(self.n)
        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]

# Cell
def collate(b):
    xb, yb = zip(*b)
    return torch.stack(xb), torch.stack(yb)


class DataLoader():
    def __init__(self, ds, batcher, collate_fcn): self.ds, self.batcher, self.collate_fcn = ds, batcher, collate_fcn
    def __iter__(self):
        for b in self.batcher: yield self.collate_fcn([self.ds[i] for i in b])
    def __len__(self): return len(self.ds)

# Cell
def fit(epochs, model, optim, loss_func, train, valid):
    for epoch in range(epochs):

        model.training = True
        for xb, yb in train:
            loss = loss_func(model(xb), yb)
            loss_func.backward()
            model.backward()

            optim.step()
            optim.zero_grad()

        model.training = False
        acc, loss, epochs = 0,0,0
        for xb, yb in valid:
            pred = model(xb)
            acc += accuracy(pred, yb)
            loss += loss_func(pred, yb)
            epochs += 1
        acc /= epochs
        loss /= epochs

        print(f'Epoch {epoch+1}, Accuracy: {acc}, Loss: {loss}')

# Cell
def get_datasets():
    xt, yt, xv, yv = get_mnist()
    tr = Dataset(xt, yt)
    val = Dataset(xv, yv)
    train = DataLoader(tr, Batcher(tr, 64, True), collate)
    valid = DataLoader(val, Batcher(val, 64, False), collate)
    return train, valid