# AUTOGENERATED! DO NOT EDIT! File to edit: FullyConnectedNetwork.ipynb (unless otherwise specified).

__all__ = ['normalize', 'get_mnist', 'test_near_zero', 'get_stats', 'get_weight', 'linear', 'relu', 'lin_rel',
           'softmax', 'mse_loss', 'Module', 'Linear', 'ReLU', 'CrossSoft', 'Model']

# Cell
from .basic_operations_01 import *

# Cell
def normalize(datasets, mean=None, std=None):
    if mean is None: mean = datasets.mean()
    if std is None: std = datasets.std()

    return (datasets - mean) / std

def get_mnist():
    path = datasets.download_data(MNIST_URL, ext='.gz')
    with gzip.open(path, 'rb') as f:
        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')

    xt,yt,xv,yv = map(tensor, (x_train, y_train, x_valid, y_valid))
    return normalize(xt).float(), yt.float(), normalize(xv, xt.mean(), xt.std()).float(), yv.float()

# Cell
def test_near_zero(data, tol=1e-3): assert data.abs() < tol; print(f'Near zero: {data}')

# Cell
def get_stats(data):
    print (f'Mean: {data.mean()}')
    print (f'Std: {data.std()}')

# Cell

#Kaiming initialization with fan_in
def get_weight(in_d, out_d, final):
    if not final: return torch.randn(in_d, out_d) * math.sqrt(2. / in_d)
    else: return torch.randn(in_d, out_d) / math.sqrt(in_d)

def linear(x, w, b): return x @ w + b

# Cell
def relu(x): return x.clamp_min(0.) - 0.5

def lin_rel(x, w, b): return relu(linear(x, w, b))

def softmax(x): return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)

# Cell
def mse_loss(xb, yb): return (xb.squeeze(-1) - yb).pow(2).mean()

# Cell

class Module():
    def __call__(self, *args):
        self.args = args
        self.out = self.forward(*args)
        return self.out

    def forward(self): raise Exception("Not Implemented")

    def backward(self): self.bwd(self.out, *self.args)

# Cell
class Linear(Module):
    def __init__(self, in_d, out_d, final):
        self.w, self.b = get_weight(in_d, out_d, final), torch.zeros(out_d)

    def forward(self, xb): return xb @ self.w + self.b

    def bwd(self, out, inp):
        inp.g = out.g @ self.w.t()
        self.w.g = inp.t() @ out.g
        self.b.g = out.g.sum(0)

# Cell
class ReLU(Module):
    def forward(self, x): return x.clamp_min_(0.)-0.5

    def bwd(self, out, inp):
        inp.g = (inp>0).float() * out.g

# Cell
class CrossSoft(Module):
    def forward(self, inp, targ):
        softed = self.soft_forward(inp)
        return self.cross_loss(softed, targ)

    def soft_forward(self, x): return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)

    def cross_loss(self, xb, targ): return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )

    def bwd(self, loss, inp, targ):
        targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)
        inp_s = softmax(inp)
        inp.g = ( inp_s - targ ) / targ.shape[0]

# Cell
class Model():
    def __init__(self, layers):
        self.layers = layers

    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

    def backward(self):
        for l in reversed(self.layers): l.backward()