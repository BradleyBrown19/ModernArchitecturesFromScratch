{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp cycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromPyTorch.nb_XResNet import *\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "> Implementing cycleGAN for unsupervised image to image domain translation. See https://junyanz.github.io/CycleGAN/.\n",
    "![CycleGAN diagram](https://hardikbansal.github.io/CycleGANBlog/images/model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoTransConv(nn.Module):\n",
    "    \"Automatic padding of transpose convolution for input-output feature size\"\n",
    "    def __init__(self, n_in, n_out, ks, stride, bias=True):\n",
    "        super().__init__()\n",
    "        padding = ks // 2\n",
    "        self.conv = nn.ConvTranspose2d(n_in, n_out, ks, stride, padding=padding, output_padding=padding, bias=bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trans_conv_norm_relu(n_in, n_out, norm_layer, bias, ks=3, stride=2):\n",
    "    \"Transpose convolutional layer\"\n",
    "    return [AutoTransConv(n_in, n_out, ks=ks, stride=stride, bias=bias),\n",
    "                         norm_layer(n_out),\n",
    "                         nn.ReLU()\n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_conv_norm_relu(n_in, n_out, norm_layer, padding_mode=\"zeros\", pad=1, ks=3, stride=1, activ=True, bias=True):\n",
    "    \"Adding ability to specify different paddings to convolutional layer\"\n",
    "    layers = []\n",
    "    if padding_mode != \"zeros\":\n",
    "        pad = 0\n",
    "        if padding_mode == \"reflection\": layers.append(nn.ReflectionPad2d(pad))\n",
    "        elif padding_mode == \"border\": layers.append(nn.ReplicationPad2d(pad))\n",
    "\n",
    "    layers.append(AutoConv(n_in, n_out, ks, stride=stride, padding_mode=padding_mode, bias=bias))\n",
    "    layers.append(norm_layer(n_out))\n",
    "    \n",
    "    if activ: layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def conv_norm_relu(n_in, n_out, norm_layer=None, ks=3, bias:bool=True, pad=1, stride=1, activ=True, a=0.2):\n",
    "    \"Convolutional layer\"\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(n_in, n_out, ks, stride=stride, padding=pad))\n",
    "    if norm_layer != None: layers.append(norm_layer(n_out))\n",
    "    if activ: layers.append(nn.LeakyReLU(a, True))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_mode, bias, dropout, norm_layer=nn.InstanceNorm2d):\n",
    "        \"Residual connections for middle section of generator\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        layers += pad_conv_norm_relu(dim, dim, norm_layer, padding_mode, bias=bias)\n",
    "        if dropout > 0: layers.append(nn.Dropout(dropout))\n",
    "        layers += (pad_conv_norm_relu(dim, dim, norm_layer, padding_mode, bias=bias, activ=False))\n",
    "        self.xb = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, xb): return xb + self.conv(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def generator(n_in, n_out, n_f=64, norm_layer=None, dropout=0., n_blocks=6, pad_mode=\"reflection\"):\n",
    "    \"Generator that maps an input of one domain to the other\"\n",
    "    norm_layer = norm_layer if norm_layer is not None else nn.InstanceNorm2d\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    layers += pad_conv_norm_relu(n_in, n_f, norm_layer, pad_mode, pad=3, ks=7, bias=bias)\n",
    "    for i in range(2): \n",
    "        layers += pad_conv_norm_relu(n_f, n_f*2, norm_layer, 'zeros', stride=2, bias=bias)\n",
    "        n_f*=2\n",
    "    \n",
    "    layers += [ResBlock(n_f, pad_mode, bias, dropout, norm_layer) for _ in range(n_blocks)]\n",
    "\n",
    "    for i in range(2):\n",
    "        layers += trans_conv_norm_relu(n_f, n_f//2, norm_layer, bias=bias)\n",
    "        n_f //= 2\n",
    "\n",
    "    layers.append(nn.ReflectionPad2d(3))\n",
    "    layers.append(nn.Conv2d(n_f, n_out, kernel_size=7, padding=0))\n",
    "    layers.append(nn.Tanh())\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "  (1): AutoConv(\n",
       "    (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflection)\n",
       "  )\n",
       "  (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): AutoConv(\n",
       "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): AutoConv(\n",
       "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       "  (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (11): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (12): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (13): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (14): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (15): ResBlock(\n",
       "    (xb): Sequential(\n",
       "      (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (1): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "      (5): AutoConv(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "      )\n",
       "      (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    )\n",
       "  )\n",
       "  (16): AutoTransConv(\n",
       "    (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (17): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (18): ReLU()\n",
       "  (19): AutoTransConv(\n",
       "    (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "  )\n",
       "  (20): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (21): ReLU()\n",
       "  (22): ReflectionPad2d((3, 3, 3, 3))\n",
       "  (23): Conv2d(64, 10, kernel_size=(7, 7), stride=(1, 1))\n",
       "  (24): Tanh()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def discriminator(c_in, n_f, n_layers, norm_layer=None, sigmoid=False):\n",
    "    \"Discrminator to classify input as belonging to one class or the other\"\n",
    "    norm_layer = nn.InstanceNorm2d if norm_layer is None else norm_layer\n",
    "    bias = (norm_layer == nn.InstanceNorm2d)\n",
    "    layers = []\n",
    "    layers += (conv_norm_relu(c_in, n_f, ks=4, stride=2, pad=1))\n",
    "\n",
    "    for i in range(n_layers-1):\n",
    "        new_f = 2*n_f if i <= 3 else n_f\n",
    "        layers += (conv_norm_relu(n_f, new_f, norm_layer, ks=4, stride=2, pad=1, bias=bias))\n",
    "        n_f = new_f\n",
    "    \n",
    "    new_f = 2*n_f if n_layers <= 3 else n_f\n",
    "\n",
    "    layers += (conv_norm_relu(n_f, new_f, norm_layer, ks=4, stride=1, pad=1, bias=bias))\n",
    "    layers.append(nn.Conv2d(new_f, 1, kernel_size=4, stride=1, padding=1))\n",
    "    if sigmoid: layers.append(nn.Sigmoid())\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 6, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (2): Conv2d(6, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (3): InstanceNorm2d(12, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (5): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  (6): InstanceNorm2d(24, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (8): Conv2d(24, 48, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  (9): InstanceNorm2d(48, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  (11): Conv2d(48, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator(3, 6, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class cycleGAN(nn.Module):\n",
    "    def __init__(self, c_in, c_out, n_f=64, disc_layers=3, gen_blocks=6, drop=0., norm_layer=None, sigmoid=False):\n",
    "        super().__init__()\n",
    "        self.a_discriminator = discriminator(c_in, n_f, disc_layers, norm_layer, sigmoid)\n",
    "        self.b_discriminator = discriminator(c_in, n_f, disc_layers, norm_layer, sigmoid)\n",
    "        self.generate_a = generator(c_in, c_out, n_f, norm_layer, drop, gen_blocks)\n",
    "        self.generate_b = generator(c_in, c_out, n_f, norm_layer, drop, gen_blocks)\n",
    "    \n",
    "    def forward(self, real_A, real_B):\n",
    "        generated_a, generated_B = self.generate_a(real_B), self.generate_b(real_A)\n",
    "        if not self.training: return generated_a, generated_b\n",
    "        id_a, id_b = self.generate_a(real_A), self.generate_b(real_B)\n",
    "        return [generated_a, generated_b, id_a, id_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DynamicLoss(nn.Module):\n",
    "    def __init__(self, loss_fn):\n",
    "        \"Loss allowing for dynamic resizing of prediction based on shape of output\"\n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "    def forward(self, pred, targ, **kwargs):\n",
    "        targ = output.new_ones(*pred.shape) if targ == 1 else output.new_zeros(*pred.shape)\n",
    "        return self.loss_fn(pred, targ, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CycleGANLoss(nn.Module):\n",
    "    def __init__(self, model, loss_fn=F.mse_loss, la=10., lb=10, lid=0.5):\n",
    "        \"CycleGAN loss\"\n",
    "        super().__init__()\n",
    "        self.model,self.la,self.lb,self.lid = model,la,lb,lid\n",
    "        self.loss_fn = DynamicLoss(loss_fn)\n",
    "    \n",
    "    def store_inputs(self, inputs):\n",
    "        self.reala,self.realb = inputs\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        gena, genb, ida, idb = pred\n",
    "\n",
    "        self.id_loss = self.lid * (self.la * F.l1_loss(ida, self.reala) + self.lb * F.l1_loss(idb,self.realb))\n",
    "\n",
    "        self.gen_loss = self.crit(self.model.a_discriminator(gena), True) + self.crit(self.model.b_discriminator(genb), True)\n",
    "\n",
    "        self.cyc_loss = self.la*  F.l1_loss(self.model.generate_a(genb), self.reala) + self.lb*F.l1_loss(self.model.generate_b(gena), self.realb)\n",
    " \n",
    "        return self.id_loss+ self.gen_loss + self.cyc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN loss is composed of 3 parts:\n",
    "1. Identity, an image that has gone through its own domain should remain the same\n",
    "2. Generator, the output images should fool the discriminator into thinking they belong to that class\n",
    "3. Cyclical loss, and image that has been mapped to the other domain then mapped back to itself should resemble the original input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class cycleGANTrainer(Callback):\n",
    "    \"Trainer to sequence timing of training both the discriminator as well as the generator's\"\n",
    "    _order = -20\n",
    "\n",
    "    def set_grad(self, da=False, db=False):\n",
    "        in_gen = (not da) and (not db)\n",
    "        requires_grad(self.learn.model.generate_a, in_gen)\n",
    "        requires_grad(self.learn.generate_b, in_gen)\n",
    "        requires_grad(self.learn.a_discriminator, da)\n",
    "        requires_grad(self.learn.b_discriminator, db)\n",
    "        if not gen:\n",
    "            self.opt_D_A.lr, self.opt_D_A.mom = self.learn.opt.lr, self.learn.opt.mom\n",
    "            self.opt_D_A.wd, self.opt_D_A.beta = self.learn.opt.wd, self.learn.opt.beta\n",
    "            self.opt_D_B.lr, self.opt_D_B.mom = self.learn.opt.lr, self.learn.opt.mom\n",
    "            self.opt_D_B.wd, self.opt_D_B.beta = self.learn.opt.wd, self.learn.opt.beta\n",
    "    \n",
    "    def before_fit(self, **kwargs):\n",
    "        self.ga = self.learn.model.generate_a\n",
    "        self.gb = self.learn.generate_b\n",
    "        self.da = self.learn.a_discriminator\n",
    "        self.db = self.learn.b_discriminator\n",
    "        self.loss_fn = self.learn.loss_func.loss_func\n",
    "\n",
    "        if not getattr(self,'opt_gen',None):\n",
    "            self.opt_gen = self.learn.opt.new([nn.Sequential(*flatten_model(self.ga), *flatten_model(self.gb))])\n",
    "        else: \n",
    "            self.opt_gen.lr,self.opt_gen.wd = self.opt.lr,self.opt.wd\n",
    "            self.opt_gen.mom,self.opt_gen.beta = self.opt.mom,self.opt.beta\n",
    "\n",
    "        if not getattr(self,'opt_da',None):\n",
    "            self.opt_da = self.learn.opt.new([nn.Sequential(*flatten_model(self.da))])\n",
    "\n",
    "        if not getattr(self,'opt_db',None):\n",
    "            self.opt_db = self.learn.opt.new([nn.Sequential(*flatten_model(self.db))])\n",
    "        \n",
    "        self.learn.opt.opt = self.opt_gen\n",
    "        self.set_grad()\n",
    "    \n",
    "    def before_batch(self, last_input, **kwargs):\n",
    "        self.learn.loss_func.store_inputs(last_input)\n",
    "    \n",
    "    def after_batch(self, last_input, last_output, **kwags):\n",
    "        #Discriminator loss\n",
    "        self.ga.zero_grad(), self.gb.zero_grad()\n",
    "        fakea, fakeb = last_output[0].detach(), last_output[1].detach()\n",
    "\n",
    "        reala,realb = last_input\n",
    "        self.set_grad(da=True)\n",
    "        self.da.zero_grad()\n",
    "        lossda = 0.5 * (self.loss_fn(self.da(reala), True) + self.loss_fn(self.da(fakea), False))\n",
    "        lossda.backward()\n",
    "        self.opt_da.step()\n",
    "\n",
    "        self.set_grad(db=True)\n",
    "        self.opt_db.zero_grad()\n",
    "        lossdb = 0.5 * (self.loss_fn(self.db(realb), True) + self.loss_fn(self.da(fakeb), False))\n",
    "        lossdb.backward()\n",
    "        self.opt_db.step()\n",
    "\n",
    "        self.set_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgan = cycleGAN(3, 3, gen_blocks=9)\n",
    "learn = get_learner(cgan, loss=CycleGANLoss(cgan))\n",
    "run = get_runner(learn, [cycleGANTrainer()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cycleGAN(\n",
       "  (a_discriminator): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (b_discriminator): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (generate_a): Sequential(\n",
       "    (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "    (1): AutoConv(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflection)\n",
       "    )\n",
       "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): AutoConv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AutoConv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (19): AutoTransConv(\n",
       "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (21): ReLU()\n",
       "    (22): AutoTransConv(\n",
       "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (24): ReLU()\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       "  (generate_b): Sequential(\n",
       "    (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "    (1): AutoConv(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), padding_mode=reflection)\n",
       "    )\n",
       "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): AutoConv(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): AutoConv(\n",
       "      (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (11): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (12): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (13): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (14): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (15): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (16): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (17): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (18): ResBlock(\n",
       "      (xb): Sequential(\n",
       "        (0): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (1): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (2): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): ReflectionPad2d((0, 0, 0, 0))\n",
       "        (5): AutoConv(\n",
       "          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflection)\n",
       "        )\n",
       "        (6): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (19): AutoTransConv(\n",
       "      (conv): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "    (20): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (21): ReLU()\n",
       "    (22): AutoTransConv(\n",
       "      (conv): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
       "    )\n",
       "    (23): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (24): ReLU()\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
