{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp optimizers_07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from ModernArchitecturesFromScratch.basic_operations_01 import *\n",
    "from ModernArchitecturesFromScratch.fully_connected_network_02 import *\n",
    "from ModernArchitecturesFromScratch.model_training_03 import *\n",
    "from ModernArchitecturesFromScratch.convolutions_pooling_04 import *\n",
    "from ModernArchitecturesFromScratch.callbacks_05 import *\n",
    "from ModernArchitecturesFromScratch.batchnorm_06 import *\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Parameter():\n",
    "    \"Defines a base class for all parameters that need to be learned by the model\"\n",
    "    def __init__(self, data=None, requires_grad=True):\n",
    "        self.d = torch.Tensor() if data is None else data\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = 0.\n",
    "    \n",
    "    def step(self, lr): \n",
    "        self.d -= lr * self.grad\n",
    "    \n",
    "    def zero_grad(self): self.grad = 0.\n",
    "    \n",
    "    def update(self, amount): self.grad = amount\n",
    "        \n",
    "    def __get__(self, instance, owner): return self.d\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return f'Parameter: {self.d.shape}, Requires grad: {self.requires_grad}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    \"Perofms a basic sgd step for the optimizer\"\n",
    "    p.d -= lr * p.grad\n",
    "\n",
    "def l2_reg_step(p, wd,**kwargs):\n",
    "    \"Adds weight decay regularization to the gradients\"\n",
    "    p.grad += wd * p.d\n",
    "\n",
    "def update_defaults(defaults, passed):\n",
    "    \"A way to update the default hyperparameters of the network\"\n",
    "    for ok,ov in defaults.items():\n",
    "        if ok not in passed:\n",
    "            passed[ok] = ov\n",
    "    return passed\n",
    "        \n",
    "    \n",
    "class NewOptimizer():\n",
    "    \"Optimizer to handle different step functions and hyperparameters\"\n",
    "    def __init__(self, params, step_fcns=[sgd_step], **defaults):\n",
    "        _defaults = {'lr':0.1, 'wd':1e-4}\n",
    "        defaults = update_defaults(_defaults, defaults)\n",
    "        \n",
    "        #Make params a list of lists\n",
    "        self.params = list(params)\n",
    "        if not isinstance(self.params[0],list): self.params = [self.params]\n",
    "            \n",
    "        self.hypes = [{**defaults} for p in self.params]\n",
    "        self.step_fcns = step_fcns\n",
    "\n",
    "    def step(self):\n",
    "        for pg,hype in zip(self.params,self.hypes): \n",
    "            for p in pg:\n",
    "                for step in self.step_fcns:\n",
    "                    step(p, **hype)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for pg in self.params: \n",
    "            for p in pg:\n",
    "                p.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Learner():\n",
    "    def __init__(self, model, loss_func, optimizer, db, **kwargs):\n",
    "        \"Wrapper for model, loss function, optimizer and databunch\"\n",
    "        self.model, self.loss_func, self.optimizer, self.db = model, loss_func, optimizer(model.parameters(), **kwargs), db\n",
    "    \n",
    "    def __repr__(self): return f'Data: \\n {self.db} \\n Model: \\n {self.model}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,_,lf = get_linear_model(0.1)\n",
    "o = NewOptimizer\n",
    "db = get_mnist_databunch()\n",
    "learn = Learner(m,lf,o,db, step_fcns=[l2_reg_step, sgd_step])\n",
    "run = Runner(learn,[Stats([accuracy]), ProgressCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_default_states(stats, state, init):\n",
    "    \"A way to populate a dictionary with default states\"\n",
    "    for stat in stats:\n",
    "        for k,v in init(stat).items():\n",
    "            state[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class StatedOptimizer():\n",
    "    \"Optimizer retrofited to have states to be kept track of by the parameters\"\n",
    "    def __init__(self, params, step_fcns=[sgd_step], stats=[], **defaults):\n",
    "        _defaults = {'lr':0.1, 'wd':1e-4, 'mom':0.9, 'sqr_mom': 0.99}\n",
    "        defaults = update_defaults(_defaults, defaults)\n",
    "        \n",
    "        #Make params a list of lists\n",
    "        self.params = list(params)\n",
    "        if not isinstance(self.params[0],list): self.params = [self.params]\n",
    "            \n",
    "        self.stats = stats\n",
    "        self.state = {}\n",
    "        \n",
    "        self.hypes = [{**defaults} for p in self.params]\n",
    "        self.step_fcns = step_fcns\n",
    "\n",
    "    def step(self):\n",
    "        i = 0\n",
    "        for p,hype in self.grad_params(): \n",
    "            if p not in self.state:\n",
    "                self.state[p] = {}\n",
    "                update_default_states(self.stats, self.state[p], lambda o: o.init_state(p))\n",
    "\n",
    "            state = self.state[p]\n",
    "            for stat in self.stats: \n",
    "                state = stat.update(p, state, **hype)\n",
    "                \n",
    "            for step in self.step_fcns: step(p, **state, **hype)\n",
    "                \n",
    "            self.state[p] = state\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for pg in self.params: \n",
    "            for p in pg:\n",
    "                p.zero_grad()\n",
    "    \n",
    "    def grad_params(self):\n",
    "        return [(p,hyper) for pg,hyper in zip(self.params,self.hypes)\n",
    "            for p in pg if p.grad is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimStat():\n",
    "    \"Base class for stats to be kept track of in the optimizer\"\n",
    "    def init(self, pg): raise NotImplementedError\n",
    "    def update(self, pg, state, **kwargs): raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageGrad(OptimStat):\n",
    "    \"Keeps track of the exponentially weighted moving average of the gradients\"\n",
    "    def init_state(self, p): return {'grad_avg': torch.zeros(p.d.shape)}\n",
    "    \n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        state[\"grad_avg\"] = state[\"grad_avg\"] * mom + (1-mom)*p.grad\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    \"Does a step of the learning rate based on the exponential moving average of the gradient\"\n",
    "    p.d -= lr * grad_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,_,lf = get_linear_model(0.1)\n",
    "o = StatedOptimizer\n",
    "db = get_mnist_databunch()\n",
    "learn = Learner(m,lf,o,db, step_fcns=[momentum_step, l2_reg_step], stats=[AverageGrad()])\n",
    "run = Runner(learn,[Stats([accuracy]), ProgressCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Dampening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OptimCounter(OptimStat):\n",
    "    \"Keeps track of how many optimizer steps were taken\"\n",
    "    def init_state(self,p): return {'steps_taken': 0}\n",
    "    def update(self, p, state, **kwargs):\n",
    "        state['steps_taken'] += 1\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageGrad(OptimStat):\n",
    "    \"Keeps track of the exponentially weighted moving average of the gradients\"\n",
    "    def __init__(self, dampening:bool=False): self.damp = dampening\n",
    "    \n",
    "    def init_state(self, p): return {'grad_avg': torch.zeros(p.d.shape)}\n",
    "    \n",
    "    def update(self, p, state, mom, **kwargs):\n",
    "        state['damp_mom'] = 1-mom if self.damp else 1. \n",
    "        #state[\"grad_avg\"] = state[\"grad_avg\"] * mom + state['damp_mom']*p.grad\n",
    "        state['grad_avg'].mul_(mom).add_(state['damp_mom'], p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AverageSquaredGrad(OptimStat):\n",
    "    \"Keeps track of the exponentially weighted moving average of the gradients\"\n",
    "    def __init__(self, dampening:bool=True): self.damp = dampening\n",
    "    \n",
    "    def init_state(self, p): return {'sqr_grad_avg': torch.zeros(p.d.shape)}\n",
    "    \n",
    "    def update(self, p, state, sqr_mom, **kwargs):\n",
    "        state['sqr_damp_mom'] = 1-sqr_mom if self.damp else 1. \n",
    "        #state[\"sqr_grad_avg\"] = state[\"grad_avg\"] * sqr_mom + state['sqr_damp_mom']*(p.grad**2)\n",
    "        state['sqr_grad_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp_mom'], p.grad.data, p.grad.data)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def debias(mom, damp, step): \n",
    "    \"Debiases the terms if momentum is or isn't used\"\n",
    "    return damp * (1 - mom**step) / (1-mom)\n",
    "\n",
    "def adam_step(p, lr, mom, damp_mom, steps_taken, sqr_mom, sqr_damp_mom, grad_avg, sqr_grad_avg, eps=1e-5, **kwargs):\n",
    "    \"Performs an Adam step of the optimizer\"\n",
    "    debias1 = debias(mom, damp_mom, steps_taken)\n",
    "    debias2 = debias(sqr_mom, sqr_damp_mom, steps_taken)\n",
    "    p.d.addcdiv_(-lr / debias1, grad_avg, (sqr_grad_avg/debias2).sqrt() + eps)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from functools import partial\n",
    "def adam_opt(beta1=0.9, beta2=0.99):\n",
    "    \"Returns an adam optimizer with momentum parameters beta1 and beta2\"\n",
    "    return partial(StatedOptimizer, step_fcns=[adam_step,l2_reg_step],\n",
    "                   stats=[AverageGrad(dampening=True), AverageSquaredGrad(), OptimCounter()], mom=beta1, sqr_mom=beta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#export\n",
    "#export\n",
    "class Runner():\n",
    "    \"All encompossing class to train a model with specific callbacks\"\n",
    "    def __init__(self, learner, cbs=None):\n",
    "        cbs = [] if cbs is None else cbs\n",
    "        self.stop,self.cbs = False,[TrainEvalCallback()]+cbs\n",
    "        \n",
    "        for cb in self.cbs:\n",
    "            cb.runner = self\n",
    "            \n",
    "        self.learner = learner\n",
    "    \n",
    "    @property\n",
    "    def model(self): return self.learner.model\n",
    "    @property\n",
    "    def optimizer(self): return self.learner.optimizer\n",
    "    @property\n",
    "    def loss_func(self): return self.learner.loss_func\n",
    "    @property\n",
    "    def databunch(self): return self.learner.db\n",
    "    @property\n",
    "    def dl(self): return self.learner.db.train if self.model.training else self.learner.db.valid\n",
    "    \n",
    "    def do_one_batch(self, xb, yb):\n",
    "        \"Applies forward and backward passes of model to one batch\"\n",
    "        self.xb, self.yb = xb, yb\n",
    "        \n",
    "        self.pred = self.learner.model(xb)\n",
    "        self.loss = self.learner.loss_func(self.pred, yb)\n",
    "        if self.check_callbacks('after_loss') or not self.learner.model.training: return\n",
    "        \n",
    "        self.learner.loss_func.backward()\n",
    "        if self.check_callbacks('after_loss_back'): return\n",
    "        \n",
    "        self.learner.model.backward()\n",
    "        if self.check_callbacks('after_model_back'): return\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        if self.check_callbacks('after_opt'): return\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        if self.check_callbacks('after_zero_grad'): return\n",
    "    \n",
    "    def do_all_batches(self, dl):\n",
    "        \"Runs every batch of a dataloader through `do_one_batch`\"\n",
    "        self.iters, self.iters_done = len(dl), 0\n",
    "        for xb, yb in dl:\n",
    "            if self.stop: break\n",
    "            self.xb, self.yb = xb,yb\n",
    "            if self.check_callbacks('before_batch'): return\n",
    "            self.do_one_batch(self.xb,self.yb)\n",
    "            if self.check_callbacks('after_batch'): return\n",
    "        self.iters = 0\n",
    "            \n",
    "        self.stop = False\n",
    "\n",
    "    def fit(self, epochs, lr=0.1):\n",
    "        \"Method to fit the model `epoch` times using learning rate `lr`\"\n",
    "        for hype in self.optimizer.hypes: hype['lr'] = lr\n",
    "        self.epochs = epochs\n",
    "        if self.check_callbacks('before_fit'): return\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.epoch = epoch\n",
    "            if self.check_callbacks('before_epoch'): return\n",
    "            if not self.check_callbacks('before_train'): self.do_all_batches(self.learner.db.train)\n",
    "            if not self.check_callbacks('before_valid'): self.do_all_batches(self.learner.db.valid)\n",
    "            if self.check_callbacks('after_epoch'): break\n",
    "        \n",
    "        if self.check_callbacks('after_fit'): return\n",
    "    \n",
    "    def check_callbacks(self, state):\n",
    "        \"Helper functions to run through each callback, calling it's state method if applicable\"\n",
    "        for cb in sorted(self.cbs, key=lambda x: x._order):\n",
    "            f = getattr(cb, state, None)\n",
    "            if f and f(): return True\n",
    "        return False\n",
    "    \n",
    "    def __repr__(self): return f'{self.learner} \\n Callbacks: \\n {self.cbs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m,lf = get_conv_model(), CrossEntropy()\n",
    "o = adam_opt()\n",
    "db = get_mnist_databunch()\n",
    "learn = Learner(m,lf,o,db)\n",
    "run = Runner(learn,[Stats([accuracy]), ProgressCallback(), HyperRecorder(['lr'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.fit(1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.cbs[3].plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
