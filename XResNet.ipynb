{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp XResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from nbdev.showdoc import show_doc\n",
    "from ModernArchitecturesFromPyTorch.nb_ScratchToPytorch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XResNet with PyTorch\n",
    "> Implementing the second generation ResNet with some tweaks presented in: https://arxiv.org/abs/1812.01187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImageNet Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we've been using MNIST for a little too long, to really test out the models going forward we're going to need to start using some bigger and harder datasets. Opting for another class, let's get Imagenet setup using FastAI's DataBlock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_imaggenette():\n",
    "    \"Grabs imagenette dataset\"\n",
    "    path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)\n",
    "    tfms = get_transforms()\n",
    "    return (ImageList.from_folder(path).split_by_rand_pct(0.2).label_from_folder().transform(tfms, size=128).databunch(bs=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental ResNet Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Identity(nn.Module):\n",
    "    \"Layer that doesn't do anything\"\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, xb): return xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AutoConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):\n",
    "        \"Automatically resizing convolution so output feature map size is same as input\"\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding, **kwargs)\n",
    "    \n",
    "    def forward(self, xb): return self.conv(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ConvBatchLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, Activation=Identity, **kwargs):\n",
    "        \"Convolutional layer with normalization and non linearity\"\n",
    "        super().__init__()\n",
    "        self.conv = AutoConv(in_channels, out_channels, kernel_size, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = Activation()\n",
    "    \n",
    "    def forward(self, xb): return self.activation(self.bn(self.conv(xb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseRes(nn.Module):\n",
    "    def __init__(self, expansion, n_in, n_h, stride=1, Activation=nn.ReLU, **kwargs):\n",
    "        \"Base class for a residual layer, can be implemented as bottleneck or normal\"\n",
    "        super().__init__()\n",
    "\n",
    "        n_out, n_in = n_h*expansion, n_in*expansion\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBatchLayer(n_in, n_h, kernel_size=1, stride=1),\n",
    "            Activation(),\n",
    "            Conv(n_h, n_h, kernel_size=3, stride=stride),\n",
    "            Activation(),\n",
    "            ConvBatchLayer(n_h, n_out, kernel_size=1, stride=1)\n",
    "        ) if expansion != 1 else nn.Sequential(\n",
    "                ConvBatchLayer(n_in, n_h, kernel_size=3, stride=stride),\n",
    "                Activation(),\n",
    "                ConvBatchLayer(n_h, n_out, kernel_size=3, stride=1),\n",
    "            )\n",
    "        \n",
    "        self.identity = Identity() if n_in == n_out else nn.Sequential(\n",
    "                    nn.AvgPool2d(kernel_size=2),\n",
    "                    ConvBatchLayer(n_in, n_out, stride=1, kernel_size=1)\n",
    "                )\n",
    "        \n",
    "        self.activation = Activation()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.activation(self.identity(xb) + self.blocks(xb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class XResnet(nn.Module):\n",
    "    def __init__(self, expansion, c_in, c_out, layer_depths, **kwargs):\n",
    "        \"XResNet in it's full glory\"\n",
    "        super().__init__()\n",
    "\n",
    "        gate_depths = [c_in, (c_in+1)*8, 64, 64]\n",
    "        gate_sizes = list(zip(gate_depths, gate_depths[1:]))\n",
    "\n",
    "        gate = [ConvBatchLayer(in_c, out_c, stride=1 if i != 0 else 2) for\n",
    "                i, (in_c, out_c) in enumerate(gate_sizes)]\n",
    "        gate.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "        body_sizes = [64//expansion,64,128,256,512]\n",
    "        res_layers = [self.sequential_res_blocks(expansion, body_sizes[i], body_sizes[i+1],\n",
    "                                      n_block=l, stride=1 if i==0 else 2, **kwargs)\n",
    "                  for i,l in enumerate(layer_depths)]\n",
    "        \n",
    "        decoder = [\n",
    "            nn.AdaptiveAvgPool2d(1), Flatten(),\n",
    "            nn.Linear(body_sizes[-1]*expansion, c_out),\n",
    "        ]\n",
    "\n",
    "        self.resnet = nn.Sequential(\n",
    "            *gate,\n",
    "            *res_layers,\n",
    "            *decoder\n",
    "        )\n",
    "    \n",
    "    @classmethod\n",
    "    def sequential_res_blocks(self, expansion, ni, nf, n_block, stride, **kwargs):\n",
    "        return nn.Sequential(\n",
    "            *[BaseRes(expansion, ni if i==0 else nf, nf, stride if i==0 else 1, **kwargs) for i in range(n_block)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, xb): return self.resnet(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def getResnet(size, c_in, c_out, **kwargs):\n",
    "    if size == 18: return XResnet(1, c_in, c_out, [2, 2,  2, 2], **kwargs)\n",
    "    elif size == 34: return XResnet(1, c_in, c_out, [3, 4,  6, 3], **kwargs)\n",
    "    elif size == 50: return XResnet(4, c_in, c_out, [3, 4,  6, 3], **kwargs)\n",
    "    elif size == 150: return XResnet(4, c_in, c_out, [3, 4, 23, 3], **kwargs)\n",
    "    elif size == 152: return XResnet(4, c_in, c_out, [3, 8, 36, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CudaCallback(Callback):\n",
    "    \"Quick callback to put model onto the GPU\"\n",
    "    def before_fit(self): self.model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = get_runner(get_learner(getResnet(18, 3, 10)), [ProgressCallback(), Stats([accuracy]), CudaCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.731658</td>\n",
       "      <td>0.417290</td>\n",
       "      <td>1.382680</td>\n",
       "      <td>0.539955</td>\n",
       "      <td>01:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.fit(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted XResNet.ipynb to ModernArchitecturesFromPyTorch/nb_XResNet.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py XResNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
