# AUTOGENERATED! DO NOT EDIT! File to edit: FullyConnectedNetwork.ipynb (unless otherwise specified).

__all__ = ['get_weight', 'linear', 'relu', 'lin_rel', 'softmax', 'mse_loss', 'Module', 'Linear', 'ReLU', 'CrossSoft',
           'eps', 'Model']

# Cell
from .basic_operations_01 import *
from nbdev.showdoc import *

# Cell

#Kaiming initialization with fan_in
def get_weight(in_d, out_d, relu_after):
    "Returns weight matrix of size `in_d` x `out_d` initialized using Kaiming initialization"
    if relu_after: return torch.randn(in_d, out_d) * math.sqrt(2. / in_d)
    else: return torch.randn(in_d, out_d) / math.sqrt(in_d)

# Cell
#hide
def linear(x, w, b):
    "Basic linear layer"
    return x @ w + b

# Cell
#hide
def relu(x):
    "ReLU activation function"
    return x.clamp_min(0.) - 0.

# Cell
#hide
def lin_rel(x, w, b):
    "Linear layer followed by ReLU activation on `x` with weight `w` and bias `b`"
    return relu(linear(x, w, b))

# Cell
#hide
def softmax(x):
    "Softmax activation function"
    return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)

# Cell
#hide
def mse_loss(xb, yb):
    "Mean Square Error loss"
    return (xb.squeeze(-1) - yb).pow(2).mean()

# Cell
class Module():
    "Base class for every layer operation in a sequential network"
    def __call__(self, *args):
        "Executes forward pass of module and stores result in `self.out` for backwards pass"
        self.args = args
        self.out = self.forward(*args)
        return self.out

    def forward(self):
        "Executes desired operation of module"
        raise Exception("Not Implemented")

    def backward(self):
        "Calls backwards method to find gradient with stored output of layer"
        self.bwd(self.out, *self.args)

# Cell
class Linear(Module):
    def __init__(self, in_d, out_d, final):
        "Initialize weight using 'get_weight' and bias to 0 for linear operation"
        self.w, self.b = get_weight(in_d, out_d, final), torch.zeros(out_d)

    def forward(self, xb):
        "Perform forward linear pass"
        return xb @ self.w + self.b

    def bwd(self, out, inp):
        "Gradient with respect to the forward linear layer"
        inp.g = out.g @ self.w.t()
        self.w.g = inp.t() @ out.g
        self.b.g = out.g.sum(0)

# Cell
class ReLU(Module):
    def forward(self, x):
        "Set all activations to have a minimum of zero, subtract 0.5 to maintain mean of 0"
        return x.clamp_min_(0.)-0.5

    def bwd(self, out, inp):
        "Backward with respect to the ReLU layer"
        inp.g = (inp>0).float() * out.g

# Cell
eps = 1e-9
class CrossSoft(Module):
    def forward(self, inp, targ):
        "Calls `soft_forward` and `cross_loss` on inp compared with `targ`"
        softed = self.soft_forward(inp)
        return self.cross_loss(softed, targ)

    def soft_forward(self, x):
        "Implements softmax activation function on `x`"
        return torch.exp(x) / torch.sum(torch.exp(x.unsqueeze(-1)), dim=1)

    def cross_loss(self, xb, targ):
        "Cross entropy loss of `xb` compared to `targ`"
        return -( (xb + eps).log()[range(targ.shape[0]), targ.long()].mean() )

    def bwd(self, loss, inp, targ):
        "Gradient with respect to both softmax and cross entropy loss"
        targ = torch.nn.functional.one_hot(targ.to(torch.int64), 10)
        inp_s = softmax(inp)
        inp.g = ( inp_s - targ ) / targ.shape[0]

# Cell
#hide
class Model():
    def __init__(self, layers):
        self.layers = layers

    def __call__(self, x):
        for l in self.layers: x = l(x)
        return x

    def backward(self):
        for l in reversed(self.layers): l.backward()