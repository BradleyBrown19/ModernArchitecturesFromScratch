# AUTOGENERATED! DO NOT EDIT! File to edit: ModelTraining.ipynb (unless otherwise specified).

__all__ = ['logsumexp', 'log_softmax_improved', 'nll', 'cross_entropy', 'accuracy', 'fit', 'Parameter',
           'SequentialModel', 'Module', 'CrossEntropy', 'Linear', 'ReLU', 'Optimizer', 'get_model', 'Dataset',
           'Batcher', 'collate', 'DataLoader', 'fit', 'get_datasets']

# Cell
from .basic_operations_01 import *
from .fully_connected_network_02 import *
from nbdev.showdoc import *

# Cell
def logsumexp(inp):
    "Helper function to compute log_softmax"
    a = inp.max(dim=1).values
    return a + ((inp-a[:,None]).exp().sum(-1)).log()

def log_softmax_improved(inp):
    "Improved `log_softmax` to take advantage of exponential properties"
    return inp - logsumexp(inp).unsqueeze(-1)

# Cell
def nll(inp, targ):
    "Computes near log likelihood"
    return -(inp[range(targ.shape[0]), targ.long()].mean())

# Cell
def cross_entropy(inp, targ):
    "Uses `log_softmax_improved` and `nll` to compute cross entropy loss"
    return nll((log_softmax_improved(inp)), targ)

# Cell
def accuracy(preds, targ):
    "Compute accuracy of `preds` with respect to `targ`"
    return (torch.argmax(preds, dim=1)==targ).float().mean()

# Cell
def fit(epochs, model, loss_func, train, valid):
    "Basic function to fit `model` for given number of `epochs` with `loss_func` and `train` and `valid`. Prints out accuracy after every epoch. This will become heavily refactored"
    for epoch in range(epochs):
        for batch in range(math.ceil(len(train)//bs)):
            start = batch*bs
            end = batch*bs + bs
            train_batch = train[start:end]
            valid_batch = valid[start:end]

            loss = loss_func(model(train_batch), valid_batch)
            loss_func.backward(loss, )
            model.backward()

            with torch.no_grad():
                for l in model.layers:
                    if hasattr(l, 'w'):
                        l.w -= l.w.g * lr
                        l.b   -= l.b.g   * lr
                        l.w.g = 0
                        l.b.g = 0
        print(f'Epoch {epoch+1}, Accuracy: {accuracy(model(xt), yt)}')

# Cell
class Parameter():
    "Defines a base class for all parameters that need to be learned by the model"
    def __init__(self, data=None, requires_grad=True):
        self.d = torch.Tensor() if data is None else data
        self.requires_grad = requires_grad
        self.grad = 0.

    def step(self, lr):
        self.d -= lr * self.grad

    def zero_grad(self): self.grad = 0.

    def update(self, amount): self.grad = amount

    def __get__(self, instance, owner): return self.d

    def __repr__(self):
        return f'Parameter: {self.d.shape}, Requires grad: {self.requires_grad}'

# Cell
class SequentialModel():
    "Model for executing forward and backward passes on a given list of `layers`"
    def __init__(self, *args):
        self.layers = list(args)
        self.training = True
        self.learner = None

    def set_learner(self, learner):
        self.learner = learner
        for lay in self.layers:
            lay.learner = learner

    def __repr__(self):
        "Prints out all modules of model"
        res = ["(Layer" + str(i+1) + "): " + str(m) for i,m in enumerate(self.layers)]
        return "\n".join(res)

    def __call__(self, x):
        "Execute forward pass on `x` throuh `self.layers`"
        for l in self.layers: x = l(x)
        return x

    def backward(self):
        "Execute backward pass on `x` throuh `self.layers`"
        for l in reversed(self.layers): l.backward()

    def parameters(self):
        "Get iterator over all parameters in layers of `self.layers`"
        for l in self.layers:
            for p in l.parameters(): yield p

# Cell
class Module():
    "Defines a base class for all layers of the network. Allows for easy implementation of forward and backward passes as well as updating learnable parameters"
    def __init__(self):
        self._params = {}

    def __call__(self, *args):
        self.args = args
        self.out = self.forward(*args)
        return self.out

    def forward(self): raise Exception("Not Implemented")

    def backward(self): self.bwd(self.out, *self.args)

    def __setattr__(self,k,v):
        if isinstance(v, Parameter): self._params[k] = v
        super().__setattr__(k,v)

    def parameters(self):
        for p in self._params.values(): yield p

# Cell

class CrossEntropy(Module):
    def forward(self, inp, targ):
        return cross_entropy(inp, targ)

    def bwd(self, loss, inp, targ):
        inp_s = softmax(inp)
        inp_s[range(targ.shape[0]), targ.long()] -= 1
        inp.g = inp_s / targ.shape[0]

# Cell

class Linear(Module):
    def __init__(self, in_d, out_d, relu_after, req_grad=True):
        super().__init__()
        self.w = Parameter(get_weight(in_d, out_d, relu_after), req_grad)
        self.b = Parameter(torch.zeros(out_d), req_grad)

    def forward(self, xb): return xb @ self.w.d + self.b.d

    def bwd(self, out, inp):
        inp.g = out.g @ self.w.d.t()
        self.w.update(inp.t() @ out.g)
        self.b.update(out.g.sum(0))

    def __repr__(self): return f'Linear({self.w.d.shape[0]}, {self.w.d.shape[1]})'

# Cell

class ReLU(Module):
    def forward(self, x): return x.clamp_min_(0.)-0.5
    def bwd(self, out, inp):
        inp.g = (inp>0).float() * out.g
    def __repr__(self): return f'ReLU()'

# Cell
class Optimizer():
    def __init__(self, params, lr): self.params, self.lr = list(params), lr

    def step(self):
        for p in self.params: p.step(self.lr)

    def zero_grad(self):
        for p in self.params: p.zero_grad()

# Cell
def get_model(lr):
    "Easy helper function to get basic fully connected network with optimizer and loss function, takes learning rate, `lr`, as a parameter"
    model = SequentialModel(Linear(784, 50, True), ReLU(), Linear(50, 10, False))
    loss_func = CrossEntropy()
    optimizer = Optimizer(model.parameters(), lr)
    return model, optimizer, loss_func

# Cell
class Dataset():
    "Container class to store and get input and target values from a dataset"
    def __init__(self, x, y): self.x, self.y = x, y

    def __getitem__(self, i): return self.x[i], self.y[i]

    def __len__(self): return len(self.x)

# Cell
class Batcher():
    "Wrapper for databunch class that randomizes each batch of output if `random` arg is set to trueu"
    def __init__(self, ds, bs, random):
        self.n, self.bs, self.rand = len(ds), bs, random
    def __iter__(self):
        "When iter is called, random batches of the dataset are created"
        self.idxs = torch.randperm(self.n) if self.rand else torch.arange(self.n)
        for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]

# Cell
def collate(b):
    "Combines the input lists `b` into single inpuut and target tensors"
    xb, yb = zip(*b)
    return torch.stack(xb), torch.stack(yb)


class DataLoader():
    "Refactored DataLoader to include a batcher, also collates the output of batcher into single tensor for model"
    def __init__(self, ds, batcher, collate_fcn): self.ds, self.batcher, self.collate_fcn = ds, batcher, collate_fcn
    def __iter__(self):
        for b in self.batcher: yield self.collate_fcn([self.ds[i] for i in b])
    def __len__(self): return len(self.ds)

# Cell
def fit(epochs, model, optim, loss_func, train, valid):
    "Fit function 4: Added validation loops, model training status as well as printing of some metrics"
    for epoch in range(epochs):

        model.training = True
        for xb, yb in train:
            loss = loss_func(model(xb), yb)
            loss_func.backward()
            model.backward()

            optim.step()
            optim.zero_grad()

        model.training = False
        acc, loss, epochs = 0,0,0
        for xb, yb in valid:
            pred = model(xb)
            acc += accuracy(pred, yb)
            loss += loss_func(pred, yb)
            epochs += 1
        acc /= epochs
        loss /= epochs

        print(f'Epoch {epoch+1}, Accuracy: {acc}, Loss: {loss}')

# Cell
def get_datasets():
    "Helper function to return proper dataloaders"
    xt, yt, xv, yv = get_mnist()
    tr = Dataset(xt, yt)
    val = Dataset(xv, yv)
    train = DataLoader(tr, Batcher(tr, 64, True), collate)
    valid = DataLoader(val, Batcher(val, 64, False), collate)
    return train, valid