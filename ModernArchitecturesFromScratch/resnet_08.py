# AUTOGENERATED! DO NOT EDIT! File to edit: ResNet.ipynb (unless otherwise specified).

__all__ = ['get_runner', 'NestedModel', 'TestMixingGrads', 'AutoConv', 'ConvBatch', 'Identity', 'BasicRes',
           'BasicResBlock', 'BottleneckBlock', 'ResBlock', 'ResLayer', 'ResNet', 'GetResnet']

# Cell
from .basic_operations_01 import *
from .fully_connected_network_02 import *
from .model_training_03 import *
from .convolutions_pooling_04 import *
from .callbacks_05 import *
from .batchnorm_06 import *
from .optimizers_07 import *

# Cell
def get_runner(model=None, layers=None, lf=None, callbacks=[Stats([accuracy]), ProgressCallback(), HyperRecorder(['lr'])], opt=None, db=None):
    "Helper function to get a quick runner"
    if model is None:
        model = SequentialModel(*layers) if layers is not None else get_linear_model(0.1)[0]
    lf = CrossEntropy() if lf is None else lf
    db = db if db is not None else get_mnist_databunch()
    opt = opt if opt is not None else adam_opt()
    learn = Learner(model,lf,opt,db)
    return Runner(learn, callbacks)

# Cell
class NestedModel(Module):
    "NestModel that allows for a sequential model to be called withing an outer model"
    def __init__(self):
        super().__init__()

    def forward(self,xb): return self.layers(xb)

    def bwd(self, out, inp): self.layers.backward()

    def parameters(self):
        for p in self.layers.parameters(): yield p

    def __repr__(self): return f'\nSubModel( \n{self.layers}\n)'

# Cell
class TestMixingGrads(NestedModel):
    "Test module to see if nested SequentialModels will work"
    def __init__(self):
        super().__init__()
        self.layers = SequentialModel(Linear(784, 50, True), ReLU(), Linear(50,25, False))

# Cell
class AutoConv(Conv):
    "Automatic resizing of padding based on kernel size to ensure constant dimensions of input to output"
    def __init__(self, n_in, n_out, kernel_size=3, stride=1):
        padding = Padding(kernel_size // 2)
        super().__init__(n_in, n_out, kernel_size, stride, padding=padding)

# Cell
class ConvBatch(NestedModel):
    "Performs conv then batchnorm"
    def __init__(self, n_in, n_out, kernel_size=3, stride=1, **kwargs):
        self.layers = SequentialModel(AutoConv(n_in, n_out, kernel_size, stride),
                       Batchnorm(n_out))

    def __repr__(self): return f'{self.layers.layers[0]}, {self.layers.layers[1]}'

# Cell
class Identity(Module):
    "Module to perform the identity connection (what goes in, comes out)"
    def forward(self,xb): return xb
    def bwd(self,out,inp): inp.g += out.g
    def __repr__(self): return f'Identity Connection'

# Cell
class BasicRes(Module):
    def __init__(self, n_in, n_out, expansion=1, stride=1, Activation=ReLU, *args, **kwargs):
        super().__init__()
        self.n_in, self.n_out, self.expansion, self.stride, self.Activation = n_in, n_out, expansion, stride, Activation

        self.identity = Identity() if self.do_identity else AutoConv(self.n_in, self.get_expansion, kernel_size=1, stride=2)

    def forward(self, xb):
        self.id_out = self.identity(xb)
        self.res_out = self.res_blocks(xb)
        self.out = self.id_out + self.res_out
        return self.out

    def bwd(self, out, inp):
        self.res_out.g = out.g
        self.id_out.g = out.g
        self.res_blocks.backward()
        self.identity.backward()

    @property
    def get_expansion(self): return self.n_out * self.expansion

    @property
    def do_identity(self): return self.n_in == self.n_out

    def parameters(self):
        layers = [self.res_blocks, self.identity]
        for m in layers:
            for p in m.parameters(): yield p

# Cell
class BasicResBlock(BasicRes):
    expansion=1
    "Basic ResBlock layer"
    def __init__(self, n_in, n_out, *args, **kwargs):
        super().__init__(n_in, n_out, *args, **kwargs)
        expansion = 1

        self.res_blocks = SequentialModel(
            ConvBatch(n_in, n_out, stride=self.stride),
            self.Activation(),
            ConvBatch(n_out, self.n_out*expansion)
        )


# Cell
class BottleneckBlock(BasicRes):
    expansion=4
    "Basic ResBlock layer"
    def __init__(self, n_in, n_out, *args, **kwargs):
        super().__init__(n_in, n_out, *args, **kwargs)

        self.res_blocks = SequentialModel(
            ConvBatch(n_in, n_out, kernel_size=1, stride=1),
            self.Activation(),
            ConvBatch(n_out, n_out),
            self.Activation(),
            ConvBatch(n_out, self.expansion, kernel_size=1)
        )

# Cell
class ResBlock(NestedModel):
    "Adds the final activation after the skip connection addition"
    def __init__(self, n_in, n_out, block=BasicResBlock, stride=1, kernel_size=3, Activation=ReLU, **kwargs):
        super().__init__()
        self.n_in, self.n_out, self.exp, self.ks, self.stride = n_in, n_out, block.expansion, kernel_size, stride
        self.layers = SequentialModel(block(n_in=n_in, n_out=n_out, expansion=block.expansion, kernel_size=kernel_size, stride=stride, Activation=Activation,**kwargs),
                                      Activation())

    def __repr__(self): return f'ResBlock({self.n_in}, {self.n_out*self.exp}, kernel_size={self.ks}, stride={self.stride})'


# Cell
class ResLayer(NestedModel):
    "Sequential res layers"
    def __init__(self, block, n, n_in, n_out, *args, **kwargs):
        layers = []
        self.block, self.n, self.n_in, self.n_out = block, n, n_in, n_out

        downsampling = 2 if n_in != n_out else 1

        layers = [ResBlock(n_in, n_out, block, stride=downsampling),
        *[ResBlock(n_out * block.expansion, n_out, block, stride=1) for i in range(n-1)]]

        self.layers = SequentialModel(*layers)

    def __repr__(self): return f'ResLayer(\n{self.layers}\n)'
    #def __repr__(self): return f'ResLayer(block={self.block.__name__}, num_lay={self.n}, n_in={self.n_in}, n_out={self.n_out}, downsample={})'

# Cell
class ResNet(NestedModel):
    "Class to create ResNet architectures of dynamic sizing"
    def __init__(self, block, layer_sizes=[64, 128, 256, 512], depths=[2,2,2,2], c_in=3,
               c_out=1000, im_size=(28,28), activation=ReLU, *args, **kwargs):

        self.layer_sizes = layer_sizes

        gate = [
            Reshape(c_in, im_size[0], im_size[1]),
            ConvBatch(c_in, self.layer_sizes[0], stride=2, kernel_size=7),
            activation(),
            Pool(max_pool, ks=3, stride=2, padding=Padding(1))
        ]

        self.conv_sizes = list(zip(self.layer_sizes, self.layer_sizes[1:]))
        body = [
            ResLayer(block, depths[0], self.layer_sizes[0], self.layer_sizes[0], Activation=activation, *args, **kwargs),
            *[ResLayer(block, n, n_in * block.expansion, n_out, Activation=activation)
             for (n_in,n_out), n in zip(self.conv_sizes, depths[1:])]
        ]

        tail = [
            Pool(avg_pool, ks=1, stride=1, padding=None),
            Flatten(),
            Linear(self.layer_sizes[-1]*block.expansion, c_out, relu_after=False)
        ]

        self.layers = SequentialModel(
            *[layer for layer in gate],
            *[layer for layer in body],
            *[layer for layer in tail]
        )

    def __repr__(self): return f'ResNet: \n{self.layers}'

# Cell
def GetResnet(size, c_in=3, c_out=10, *args, **kwargs):
    "Helper function to get ResNet architectures of different sizes"
    if size == 18: return ResNet(c_in=c_in, c_out=c_out, block=BasicResBlock, depths=[2, 2, 2, 2], size=size, **kwargs)
    elif size == 34: return ResNet(c_in=c_in, c_out=c_out, block=BasicResBlock, depths=[3, 4, 6, 3], size=size, **kwargs)
    elif size == 50: return ResNet(c_in=c_in, c_out=c_out, block=BottleneckBlock, depths=[3, 4, 6, 3], size=size, **kwargs)
    elif size == 150: return ResNet(c_in=c_in, c_out=c_out, block=BottleneckBlock, depths=[3, 4, 23, 3], size=size, **kwargs)
    elif size == 152: return ResNet(c_in=c_in, c_out=c_out, block=BottleneckBlock, depths=[3, 8, 36, 3], size=size, **kwargs)